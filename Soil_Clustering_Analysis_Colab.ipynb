{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üåæ Precision Farming: Soil & Environmental Data Clustering\n",
    "\n",
    "This notebook performs multivariate clustering analysis on soil and environmental data to support precision farming decisions.\n",
    "\n",
    "**Features:**\n",
    "- Multiple clustering algorithms (K-Means, DBSCAN, Hierarchical, GMM)\n",
    "- PCA dimensionality reduction\n",
    "- Interactive visualizations\n",
    "- Cluster profiling and statistics\n",
    "- Downloadable results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install"
   },
   "source": [
    "## üì¶ Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-packages"
   },
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn plotly matplotlib seaborn openpyxl -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## üìö Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import-libs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "from google.colab import files\n",
    "import io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "backend-classes"
   },
   "source": [
    "## üîß Step 3: Backend Clustering Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clustering-class"
   },
   "outputs": [],
   "source": [
    "class SoilClusteringEngine:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = None\n",
    "        self.models = {}\n",
    "        self.scaled_data = None\n",
    "        self.pca_data = None\n",
    "        self.results = {}\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def preprocess_data(self, df, feature_columns):\n",
    "        \"\"\"Standardize the data\"\"\"\n",
    "        self.feature_names = feature_columns\n",
    "        data = df[feature_columns].values\n",
    "        self.scaled_data = self.scaler.fit_transform(data)\n",
    "        return self.scaled_data\n",
    "    \n",
    "    def apply_pca(self, n_components=3):\n",
    "        \"\"\"Apply PCA for dimensionality reduction\"\"\"\n",
    "        if self.scaled_data is None:\n",
    "            raise ValueError(\"Data must be preprocessed first\")\n",
    "        \n",
    "        self.pca = PCA(n_components=n_components)\n",
    "        self.pca_data = self.pca.fit_transform(self.scaled_data)\n",
    "        \n",
    "        return {\n",
    "            'pca_data': self.pca_data,\n",
    "            'variance_explained': self.pca.explained_variance_ratio_,\n",
    "            'cumulative_variance': np.cumsum(self.pca.explained_variance_ratio_)\n",
    "        }\n",
    "    \n",
    "    def calculate_elbow_scores(self, max_clusters=10):\n",
    "        \"\"\"Calculate elbow metrics for K-Means\"\"\"\n",
    "        inertias = []\n",
    "        silhouette_scores = []\n",
    "        davies_bouldin_scores = []\n",
    "        k_range = range(2, min(max_clusters + 1, len(self.scaled_data)))\n",
    "        \n",
    "        for k in k_range:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            labels = kmeans.fit_predict(self.scaled_data)\n",
    "            \n",
    "            inertias.append(kmeans.inertia_)\n",
    "            silhouette_scores.append(silhouette_score(self.scaled_data, labels))\n",
    "            davies_bouldin_scores.append(davies_bouldin_score(self.scaled_data, labels))\n",
    "        \n",
    "        return {\n",
    "            'k_values': list(k_range),\n",
    "            'inertias': inertias,\n",
    "            'silhouette_scores': silhouette_scores,\n",
    "            'davies_bouldin_scores': davies_bouldin_scores\n",
    "        }\n",
    "    \n",
    "    def perform_kmeans(self, n_clusters):\n",
    "        \"\"\"Perform K-Means clustering\"\"\"\n",
    "        model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        labels = model.fit_predict(self.scaled_data)\n",
    "        \n",
    "        return self._get_metrics('K-Means', model, labels)\n",
    "    \n",
    "    def perform_dbscan(self, eps=0.5, min_samples=5):\n",
    "        \"\"\"Perform DBSCAN clustering\"\"\"\n",
    "        model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = model.fit_predict(self.scaled_data)\n",
    "        \n",
    "        return self._get_metrics('DBSCAN', model, labels)\n",
    "    \n",
    "    def perform_hierarchical(self, n_clusters, linkage='ward'):\n",
    "        \"\"\"Perform Hierarchical clustering\"\"\"\n",
    "        model = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n",
    "        labels = model.fit_predict(self.scaled_data)\n",
    "        \n",
    "        return self._get_metrics('Hierarchical', model, labels)\n",
    "    \n",
    "    def perform_gmm(self, n_components, covariance_type='full'):\n",
    "        \"\"\"Perform Gaussian Mixture Model clustering\"\"\"\n",
    "        model = GaussianMixture(n_components=n_components, covariance_type=covariance_type, random_state=42)\n",
    "        model.fit(self.scaled_data)\n",
    "        labels = model.predict(self.scaled_data)\n",
    "        \n",
    "        result = self._get_metrics('GMM', model, labels)\n",
    "        result['bic'] = model.bic(self.scaled_data)\n",
    "        result['aic'] = model.aic(self.scaled_data)\n",
    "        return result\n",
    "    \n",
    "    def _get_metrics(self, algorithm_name, model, labels):\n",
    "        \"\"\"Calculate clustering metrics\"\"\"\n",
    "        unique_labels = set(labels)\n",
    "        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "        \n",
    "        result = {\n",
    "            'algorithm': algorithm_name,\n",
    "            'labels': labels,\n",
    "            'model': model,\n",
    "            'n_clusters': n_clusters\n",
    "        }\n",
    "        \n",
    "        if n_clusters > 1 and len(unique_labels) > 1:\n",
    "            valid_mask = labels != -1\n",
    "            if valid_mask.sum() > 0:\n",
    "                result['silhouette_score'] = silhouette_score(self.scaled_data[valid_mask], labels[valid_mask])\n",
    "                result['davies_bouldin_score'] = davies_bouldin_score(self.scaled_data[valid_mask], labels[valid_mask])\n",
    "                result['calinski_harabasz_score'] = calinski_harabasz_score(self.scaled_data[valid_mask], labels[valid_mask])\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_cluster_statistics(self, df, feature_columns, labels):\n",
    "        \"\"\"Calculate cluster statistics\"\"\"\n",
    "        df_with_clusters = df.copy()\n",
    "        df_with_clusters['Cluster'] = labels\n",
    "        \n",
    "        cluster_stats = df_with_clusters.groupby('Cluster')[feature_columns].agg(['mean', 'std', 'min', 'max']).round(3)\n",
    "        cluster_sizes = df_with_clusters['Cluster'].value_counts().sort_index()\n",
    "        \n",
    "        return {\n",
    "            'statistics': cluster_stats,\n",
    "            'sizes': cluster_sizes,\n",
    "            'df_with_clusters': df_with_clusters\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Clustering engine class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload-data"
   },
   "source": [
    "## üìÇ Step 4: Upload Your CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload"
   },
   "outputs": [],
   "source": [
    "print(\"Please upload your soil/environmental data CSV file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Load the uploaded file\n",
    "filename = list(uploaded.keys())[0]\n",
    "if filename.endswith('.csv'):\n",
    "    df = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
    "else:\n",
    "    df = pd.read_excel(io.BytesIO(uploaded[filename]))\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(df)} rows and {len(df.columns)} columns\")\n",
    "print(\"\\nüìä Data Preview:\")\n",
    "display(df.head())\n",
    "print(\"\\nüìà Statistical Summary:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "explore-data"
   },
   "source": [
    "## üîç Step 5: Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explore"
   },
   "outputs": [],
   "source": [
    "# Show all columns\n",
    "print(\"Available columns in your dataset:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i}. {col} (Type: {df[col].dtype})\")\n",
    "\n",
    "# Get numeric columns\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"\\nüî¢ Found {len(numeric_columns)} numeric columns for clustering\")\n",
    "\n",
    "# Correlation heatmap\n",
    "if len(numeric_columns) > 1:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(df[numeric_columns].corr(), annot=True, cmap='RdBu', center=0, fmt='.2f')\n",
    "    plt.title('Feature Correlation Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "select-features"
   },
   "source": [
    "## ‚úÖ Step 6: Select Features for Clustering\n",
    "\n",
    "Modify the `selected_features` list below with the column names you want to use for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "features"
   },
   "outputs": [],
   "source": [
    "# MODIFY THIS: Select features for clustering\n",
    "# Example: selected_features = ['pH', 'Nitrogen_ppm', 'Phosphorus_ppm', 'Potassium_ppm']\n",
    "selected_features = numeric_columns[:min(8, len(numeric_columns))]  # Auto-select first 8 numeric columns\n",
    "\n",
    "print(f\"Selected features for clustering: {selected_features}\")\n",
    "print(f\"Total features: {len(selected_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preprocess"
   },
   "source": [
    "## üîß Step 7: Preprocessing and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pca"
   },
   "outputs": [],
   "source": [
    "# Initialize clustering engine\n",
    "engine = SoilClusteringEngine()\n",
    "\n",
    "# Preprocess data\n",
    "scaled_data = engine.preprocess_data(df, selected_features)\n",
    "print(\"‚úÖ Data standardized\")\n",
    "\n",
    "# Apply PCA\n",
    "n_components = min(3, len(selected_features))\n",
    "pca_result = engine.apply_pca(n_components=n_components)\n",
    "\n",
    "print(f\"\\nüìä PCA Results:\")\n",
    "print(f\"Components: {n_components}\")\n",
    "print(f\"Variance explained by each component: {[f'{v*100:.2f}%' for v in pca_result['variance_explained']]}\")\n",
    "print(f\"Total variance explained: {pca_result['cumulative_variance'][-1]*100:.2f}%\")\n",
    "\n",
    "# Visualize variance explained\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.bar(range(1, n_components+1), pca_result['variance_explained']*100)\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Variance Explained (%)')\n",
    "ax1.set_title('Variance Explained by Each Component')\n",
    "\n",
    "ax2.plot(range(1, n_components+1), pca_result['cumulative_variance']*100, marker='o')\n",
    "ax2.set_xlabel('Number of Components')\n",
    "ax2.set_ylabel('Cumulative Variance (%)')\n",
    "ax2.set_title('Cumulative Variance Explained')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elbow"
   },
   "source": [
    "## üìä Step 8: Elbow Method (K-Means Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elbow-calc"
   },
   "outputs": [],
   "source": [
    "# Calculate elbow metrics\n",
    "elbow_data = engine.calculate_elbow_scores(max_clusters=10)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(elbow_data['k_values'], elbow_data['inertias'], marker='o', linewidth=2)\n",
    "axes[0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].set_title('Elbow Method (Inertia)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(elbow_data['k_values'], elbow_data['silhouette_scores'], marker='o', linewidth=2, color='green')\n",
    "axes[1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Score (Higher is Better)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(elbow_data['k_values'], elbow_data['davies_bouldin_scores'], marker='o', linewidth=2, color='red')\n",
    "axes[2].set_xlabel('Number of Clusters (k)')\n",
    "axes[2].set_ylabel('Davies-Bouldin Index')\n",
    "axes[2].set_title('Davies-Bouldin Index (Lower is Better)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Suggest optimal k\n",
    "best_k_idx = np.argmax(elbow_data['silhouette_scores'])\n",
    "suggested_k = elbow_data['k_values'][best_k_idx]\n",
    "print(f\"\\nüí° Suggested optimal clusters: {suggested_k} (based on Silhouette Score)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clustering"
   },
   "source": [
    "## üéØ Step 9: Run Clustering Algorithms\n",
    "\n",
    "Choose which algorithm(s) to run by setting the parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-clustering"
   },
   "outputs": [],
   "source": [
    "# MODIFY THESE PARAMETERS\n",
    "n_clusters = 3  # For K-Means, Hierarchical, GMM\n",
    "\n",
    "# Run all algorithms\n",
    "results = {}\n",
    "\n",
    "print(\"Running clustering algorithms...\\n\")\n",
    "\n",
    "# 1. K-Means\n",
    "print(\"1Ô∏è‚É£ K-Means...\")\n",
    "results['K-Means'] = engine.perform_kmeans(n_clusters=n_clusters)\n",
    "print(f\"   Clusters: {results['K-Means']['n_clusters']}\")\n",
    "print(f\"   Silhouette: {results['K-Means'].get('silhouette_score', 'N/A')}\")\n",
    "\n",
    "# 2. DBSCAN\n",
    "print(\"\\n2Ô∏è‚É£ DBSCAN...\")\n",
    "results['DBSCAN'] = engine.perform_dbscan(eps=0.8, min_samples=5)\n",
    "print(f\"   Clusters: {results['DBSCAN']['n_clusters']}\")\n",
    "print(f\"   Silhouette: {results['DBSCAN'].get('silhouette_score', 'N/A')}\")\n",
    "\n",
    "# 3. Hierarchical\n",
    "print(\"\\n3Ô∏è‚É£ Hierarchical...\")\n",
    "results['Hierarchical'] = engine.perform_hierarchical(n_clusters=n_clusters, linkage='ward')\n",
    "print(f\"   Clusters: {results['Hierarchical']['n_clusters']}\")\n",
    "print(f\"   Silhouette: {results['Hierarchical'].get('silhouette_score', 'N/A')}\")\n",
    "\n",
    "# 4. Gaussian Mixture Model\n",
    "print(\"\\n4Ô∏è‚É£ Gaussian Mixture Model...\")\n",
    "results['GMM'] = engine.perform_gmm(n_components=n_clusters, covariance_type='full')\n",
    "print(f\"   Clusters: {results['GMM']['n_clusters']}\")\n",
    "print(f\"   Silhouette: {results['GMM'].get('silhouette_score', 'N/A')}\")\n",
    "print(f\"   BIC: {results['GMM'].get('bic', 'N/A')}\")\n",
    "\n",
    "print(\"\\n‚úÖ All algorithms completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comparison"
   },
   "source": [
    "## üìä Step 10: Algorithm Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compare"
   },
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = []\n",
    "for algo_name, result in results.items():\n",
    "    row = {\n",
    "        'Algorithm': algo_name,\n",
    "        'Clusters': result['n_clusters'],\n",
    "        'Silhouette': round(result.get('silhouette_score', 0), 4),\n",
    "        'Davies-Bouldin': round(result.get('davies_bouldin_score', 0), 4),\n",
    "        'Calinski-Harabasz': round(result.get('calinski_harabasz_score', 0), 2)\n",
    "    }\n",
    "    if 'bic' in result:\n",
    "        row['BIC'] = round(result['bic'], 2)\n",
    "        row['AIC'] = round(result['aic'], 2)\n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nüìä Algorithm Comparison:\")\n",
    "display(comparison_df)\n",
    "\n",
    "print(\"\\nüí° Metric Interpretation:\")\n",
    "print(\"  ‚Ä¢ Silhouette Score: Higher is better (range: -1 to 1)\")\n",
    "print(\"  ‚Ä¢ Davies-Bouldin Index: Lower is better\")\n",
    "print(\"  ‚Ä¢ Calinski-Harabasz: Higher is better\")\n",
    "print(\"  ‚Ä¢ BIC/AIC (GMM only): Lower is better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualize"
   },
   "source": [
    "## üìà Step 11: Visualize Clustering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viz-3d"
   },
   "outputs": [],
   "source": [
    "# 3D Visualization for each algorithm\n",
    "if pca_result['pca_data'].shape[1] >= 3:\n",
    "    for algo_name, result in results.items():\n",
    "        df_plot = pd.DataFrame({\n",
    "            'PC1': pca_result['pca_data'][:, 0],\n",
    "            'PC2': pca_result['pca_data'][:, 1],\n",
    "            'PC3': pca_result['pca_data'][:, 2],\n",
    "            'Cluster': result['labels'].astype(str)\n",
    "        })\n",
    "        \n",
    "        fig = px.scatter_3d(\n",
    "            df_plot,\n",
    "            x='PC1', y='PC2', z='PC3',\n",
    "            color='Cluster',\n",
    "            title=f'{algo_name} - 3D Cluster Visualization',\n",
    "            labels={'PC1': 'PC1', 'PC2': 'PC2', 'PC3': 'PC3'}\n",
    "        )\n",
    "        fig.update_traces(marker=dict(size=5))\n",
    "        fig.show()\n",
    "else:\n",
    "    print(\"Not enough components for 3D visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d-viz"
   },
   "source": [
    "## üìâ 2D Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viz-2d"
   },
   "outputs": [],
   "source": [
    "# 2D scatter plots for all algorithms\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (algo_name, result) in enumerate(results.items()):\n",
    "    scatter = axes[idx].scatter(\n",
    "        pca_result['pca_data'][:, 0],\n",
    "        pca_result['pca_data'][:, 1],\n",
    "        c=result['labels'],\n",
    "        cmap='viridis',\n",
    "        s=50,\n",
    "        alpha=0.6,\n",
    "        edgecolors='white',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "    axes[idx].set_xlabel('Principal Component 1')\n",
    "    axes[idx].set_ylabel('Principal Component 2')\n",
    "    axes[idx].set_title(f'{algo_name} (n={result[\"n_clusters\"]} clusters)')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=axes[idx], label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stats"
   },
   "source": [
    "## üìã Step 12: Cluster Statistics and Profiling\n",
    "\n",
    "Choose which algorithm's results to analyze in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cluster-stats"
   },
   "outputs": [],
   "source": [
    "# MODIFY THIS: Choose algorithm for detailed analysis\n",
    "selected_algorithm = 'K-Means'  # Options: 'K-Means', 'DBSCAN', 'Hierarchical', 'GMM'\n",
    "\n",
    "selected_result = results[selected_algorithm]\n",
    "cluster_stats = engine.get_cluster_statistics(df, selected_features, selected_result['labels'])\n",
    "\n",
    "print(f\"\\nüìä Detailed Analysis for {selected_algorithm}\\n\")\n",
    "\n",
    "# Cluster sizes\n",
    "print(\"Cluster Sizes:\")\n",
    "for cluster_id, size in cluster_stats['sizes'].items():\n",
    "    percentage = (size / len(df) * 100)\n",
    "    print(f\"  Cluster {cluster_id}: {size} samples ({percentage:.1f}%)\")\n",
    "\n",
    "# Cluster statistics\n",
    "print(\"\\nCluster Statistics (Mean Values):\")\n",
    "display(cluster_stats['statistics'].xs('mean', axis=1, level=1))\n",
    "\n",
    "# Radar chart for cluster comparison\n",
    "cluster_means = cluster_stats['statistics'].xs('mean', axis=1, level=1)\n",
    "fig = go.Figure()\n",
    "\n",
    "for cluster in cluster_means.index:\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=cluster_means.loc[cluster].values,\n",
    "        theta=selected_features,\n",
    "        fill='toself',\n",
    "        name=f'Cluster {cluster}'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(radialaxis=dict(visible=True)),\n",
    "    showlegend=True,\n",
    "    title=f\"Cluster Profile Comparison - {selected_algorithm}\",\n",
    "    height=600\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## üíæ Step 13: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export"
   },
   "outputs": [],
   "source": [
    "# Save clustered data\n",
    "df_export = cluster_stats['df_with_clusters']\n",
    "df_export.to_csv('clustered_soil_data.csv', index=False)\n",
    "print(\"‚úÖ Saved: clustered_soil_data.csv\")\n",
    "\n",
    "# Save cluster statistics\n",
    "cluster_stats['statistics'].to_csv('cluster_statistics.csv')\n",
    "print(\"‚úÖ Saved: cluster_statistics.csv\")\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv('algorithm_comparison.csv', index=False)\n",
    "print(\"‚úÖ Saved: algorithm_comparison.csv\")\n",
    "\n",
    "# Download files\n",
    "print(\"\\nüì• Downloading files...\")\n",
    "files.download('clustered_soil_data.csv')\n",
    "files.download('cluster_statistics.csv')\n",
    "files.download('algorithm_comparison.csv')\n",
    "\n",
    "print(\"\\n‚úÖ All files downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## üìù Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "summary-report"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PRECISION FARMING CLUSTERING ANALYSIS REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset: {len(df)} samples, {len(selected_features)} features\")\n",
    "print(f\"Features analyzed: {', '.join(selected_features)}\")\n",
    "print(f\"\\nPCA: {n_components} components explaining {pca_result['cumulative_variance'][-1]*100:.1f}% variance\")\n",
    "print(\"\\nAlgorithm Results:\")\n",
    "for algo_name, result in results.items():\n",
    "    print(f\"\\n  {algo_name}:\")\n",
    "    print(f\"    - Clusters: {result['n_clusters']}\")\n",
    "    if 'silhouette_score' in result:\n",
    "        print(f\"    - Silhouette Score: {result['silhouette_score']:.4f}\")\n",
    "    if 'davies_bouldin_score' in result:\n",
    "        print(f\"    - Davies-Bouldin: {result['davies_bouldin_score']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Analysis complete! Check the downloaded CSV files for detailed results.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Soil_Clustering_Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
